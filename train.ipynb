{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PTT CVS Product Review Analysis\n",
    "\n",
    "This project demonstrates a system for predicting product ratings based on reviews from PTT's CVS board using machine learning, specifically fine-tuning a pre-trained model. The tokenizer model used in this project is 'bert-base-chinese' and the pre-trained model is 'ckiplab/albert-tiny-chinese-ws', a variant of ALBERT for Traditional Chinese text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tf-keras\n",
      "  Downloading tf_keras-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: tensorflow<2.19,>=2.18 in /usr/local/lib/python3.9/site-packages (from tf-keras) (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (24.12.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (3.4.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (5.29.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (58.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.69.0)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (3.8.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (2.0.2)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (3.12.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (0.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (0.37.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow<2.19,>=2.18->tf-keras) (0.45.1)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.9/site-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (13.9.4)\n",
      "Requirement already satisfied: namex in /usr/local/lib/python3.9/site-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (0.0.8)\n",
      "Requirement already satisfied: optree in /usr/local/lib/python3.9/site-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (0.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf-keras) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf-keras) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf-keras) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf-keras) (2024.12.14)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/site-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.9/site-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/site-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras) (3.1.3)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras) (8.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.9/site-packages (from rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.9/site-packages (from rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (2.19.1)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras) (3.21.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.9/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (0.1.2)\n",
      "Downloading tf_keras-2.18.0-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Installing collected packages: tf-keras\n",
      "Successfully installed tf-keras-2.18.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate==0.26.0\n",
      "  Downloading accelerate-0.26.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/site-packages (from accelerate==0.26.0) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/site-packages (from accelerate==0.26.0) (24.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.9/site-packages (from accelerate==0.26.0) (6.1.1)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.9/site-packages (from accelerate==0.26.0) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.9/site-packages (from accelerate==0.26.0) (2.5.1)\n",
      "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.9/site-packages (from accelerate==0.26.0) (0.27.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.9/site-packages (from accelerate==0.26.0) (0.5.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/site-packages (from torch>=1.10.0->accelerate==0.26.0) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.9/site-packages (from torch>=1.10.0->accelerate==0.26.0) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.9/site-packages (from torch>=1.10.0->accelerate==0.26.0) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/site-packages (from torch>=1.10.0->accelerate==0.26.0) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.9/site-packages (from torch>=1.10.0->accelerate==0.26.0) (2024.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.9/site-packages (from torch>=1.10.0->accelerate==0.26.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.9/site-packages (from torch>=1.10.0->accelerate==0.26.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.9/site-packages (from torch>=1.10.0->accelerate==0.26.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.9/site-packages (from torch>=1.10.0->accelerate==0.26.0) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.9/site-packages (from torch>=1.10.0->accelerate==0.26.0) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.9/site-packages (from torch>=1.10.0->accelerate==0.26.0) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.9/site-packages (from torch>=1.10.0->accelerate==0.26.0) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.9/site-packages (from torch>=1.10.0->accelerate==0.26.0) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.9/site-packages (from torch>=1.10.0->accelerate==0.26.0) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.9/site-packages (from torch>=1.10.0->accelerate==0.26.0) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.9/site-packages (from torch>=1.10.0->accelerate==0.26.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.9/site-packages (from torch>=1.10.0->accelerate==0.26.0) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.9/site-packages (from torch>=1.10.0->accelerate==0.26.0) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.9/site-packages (from torch>=1.10.0->accelerate==0.26.0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.9/site-packages (from sympy==1.13.1->torch>=1.10.0->accelerate==0.26.0) (1.3.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/site-packages (from huggingface-hub->accelerate==0.26.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.9/site-packages (from huggingface-hub->accelerate==0.26.0) (4.67.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/site-packages (from jinja2->torch>=1.10.0->accelerate==0.26.0) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate==0.26.0) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate==0.26.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate==0.26.0) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate==0.26.0) (2024.12.14)\n",
      "Downloading accelerate-0.26.0-py3-none-any.whl (270 kB)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-0.26.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install accelerate==0.26.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-17 08:48:28.570898: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1737103708.630358     979 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1737103708.645585     979 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-17 08:48:28.791485: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import BertTokenizerFast, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.backends\n",
    "import warnings\n",
    "from datasets import Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Exploration\n",
    "Load the data and explore its structure to ensure there are no errors or anomalies. Check the rating column's range, distribution, and other statistical details.\n",
    "\n",
    "1. **Load the Data**: Import the CSV file and display the first few rows.\n",
    "2. **Describe the Data**: Use `describe()` to get basic statistical information about the dataset.\n",
    "3. **Check Rating Range**: Ensure that all ratings are within a valid range (0 to 100).\n",
    "4. **View Rating Distribution**: Check the distribution of ratings and calculate the average rating for each store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"cvs_products.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product</th>\n",
       "      <th>link</th>\n",
       "      <th>store</th>\n",
       "      <th>CVS</th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>辻利抹茶歐蕾 38</td>\n",
       "      <td>https://www.ptt.cc/bbs/CVS/M.1570449843.A.7FF....</td>\n",
       "      <td>全家</td>\n",
       "      <td>全家</td>\n",
       "      <td>30</td>\n",
       "      <td>難喝.. 喝起來沒有抹茶味 只有一股很噁的甜味和水味 為了確認又喝幾口 真的難喝 大家就避開...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>香辣霸王雞球/原價39元（友善價28元）</td>\n",
       "      <td>https://www.ptt.cc/bbs/CVS/M.1570452664.A.B42....</td>\n",
       "      <td>全家</td>\n",
       "      <td>全家</td>\n",
       "      <td>60</td>\n",
       "      <td>基本上在便利商店買這個就是解嘴饞 （因為要吃的鹹酥雞都沒開...） 然後一如其它炸物 微波後...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>黑松C&amp;C氣泡飲(王林蘋果風味) / 原價$29 全家購入特價$22</td>\n",
       "      <td>https://www.ptt.cc/bbs/CVS/M.1570461438.A.947....</td>\n",
       "      <td>全家 / 黑松</td>\n",
       "      <td>全家</td>\n",
       "      <td>90</td>\n",
       "      <td>最近好多蘋果氣泡飲料喔 看到綠色瓶身很漂亮，又有嚐鮮價 立刻忍不住試試XD 和其他蘋果汽水比...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>美少女戰士悠遊閃卡-戰士款/100元</td>\n",
       "      <td>https://www.ptt.cc/bbs/CVS/M.1570494716.A.C35....</td>\n",
       "      <td>7-11 / 悠遊卡股份有限公司</td>\n",
       "      <td>7-11</td>\n",
       "      <td>100</td>\n",
       "      <td>第一彈其實完全沒有follow到， 偶然在板上看到第二彈情報文，結果覺得更生火！ 身為兒時禮...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>統一麵包紫薯QQ 35</td>\n",
       "      <td>https://www.ptt.cc/bbs/CVS/M.1570502732.A.406....</td>\n",
       "      <td>7-11 統一麵包</td>\n",
       "      <td>7-11</td>\n",
       "      <td>88</td>\n",
       "      <td>早上到小七挑早餐看到紫薯覺得很特別就買來吃吃看 麵包整個顏色偏紫 味道聞起來甜甜 有淡淡的紫...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              product  \\\n",
       "0                           辻利抹茶歐蕾 38   \n",
       "1                香辣霸王雞球/原價39元（友善價28元）   \n",
       "2  黑松C&C氣泡飲(王林蘋果風味) / 原價$29 全家購入特價$22   \n",
       "3                  美少女戰士悠遊閃卡-戰士款/100元   \n",
       "4                         統一麵包紫薯QQ 35   \n",
       "\n",
       "                                                link             store   CVS  \\\n",
       "0  https://www.ptt.cc/bbs/CVS/M.1570449843.A.7FF....                全家    全家   \n",
       "1  https://www.ptt.cc/bbs/CVS/M.1570452664.A.B42....                全家    全家   \n",
       "2  https://www.ptt.cc/bbs/CVS/M.1570461438.A.947....           全家 / 黑松    全家   \n",
       "3  https://www.ptt.cc/bbs/CVS/M.1570494716.A.C35....  7-11 / 悠遊卡股份有限公司  7-11   \n",
       "4  https://www.ptt.cc/bbs/CVS/M.1570502732.A.406....         7-11 統一麵包  7-11   \n",
       "\n",
       "   rating                                             review  \n",
       "0      30  難喝.. 喝起來沒有抹茶味 只有一股很噁的甜味和水味 為了確認又喝幾口 真的難喝 大家就避開...  \n",
       "1      60  基本上在便利商店買這個就是解嘴饞 （因為要吃的鹹酥雞都沒開...） 然後一如其它炸物 微波後...  \n",
       "2      90  最近好多蘋果氣泡飲料喔 看到綠色瓶身很漂亮，又有嚐鮮價 立刻忍不住試試XD 和其他蘋果汽水比...  \n",
       "3     100  第一彈其實完全沒有follow到， 偶然在板上看到第二彈情報文，結果覺得更生火！ 身為兒時禮...  \n",
       "4      88  早上到小七挑早餐看到紫薯覺得很特別就買來吃吃看 麵包整個顏色偏紫 味道聞起來甜甜 有淡淡的紫...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 30,  60,  90, 100,  88,  68,  95,  75,  85,  80,  70,  40,  98,\n",
       "        86,  65,  79,  59,  39,  89,  55,  49,  66,  83,  92,  84,  58,\n",
       "         3,  50,  87,  78,  15,  99,  61,  72,  20,  81,  82,  93,   0,\n",
       "        73,  10,  56,   7,  96,  74,  67,   8,  63,  76,   1,  69,  45,\n",
       "         5,  77,  33,  62,  48,  94,  19,  42,  35,  25,  29,  91,  23,\n",
       "        11,  32,  64,  71,  43,  31,   2,  97,  28])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['rating'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rating\n",
      "80     1835\n",
      "85     1459\n",
      "90     1184\n",
      "70      974\n",
      "75      922\n",
      "60      422\n",
      "65      374\n",
      "88      234\n",
      "95      234\n",
      "50      199\n",
      "100     189\n",
      "59      133\n",
      "82      130\n",
      "40      100\n",
      "78       99\n",
      "30       95\n",
      "83       95\n",
      "0        92\n",
      "79       56\n",
      "55       55\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "rating_counts = df['rating'].value_counts().sort_values(ascending=False)\n",
    "print(rating_counts.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CVS\n",
      "7-11    75.067762\n",
      "OK      74.958042\n",
      "全家      76.982945\n",
      "萊爾富     76.654160\n",
      "Name: rating, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "average_ratings = df.groupby('CVS')['rating'].mean()\n",
    "print(average_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>9618.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>76.253067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>16.159308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>70.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>80.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>85.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            rating\n",
       "count  9618.000000\n",
       "mean     76.253067\n",
       "std      16.159308\n",
       "min       0.000000\n",
       "25%      70.000000\n",
       "50%      80.000000\n",
       "75%      85.000000\n",
       "max     100.000000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Preprocessing\n",
    "Clean the data by removing unnecessary columns and standardizing the ratings to make the data suitable for model training.\n",
    "\n",
    "1. **Data Cleaning**: Remove columns that are not needed for the model (keep only `review` and `rating`).\n",
    "2. **Standardization**: Standardize the rating column to ensure it is scaled appropriately for regression models.\n",
    "3. **Train-Test Split**: Split the data into training (90%) and testing (10%) sets for model training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.drop(['product', 'store', 'link', 'CVS'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30</td>\n",
       "      <td>難喝.. 喝起來沒有抹茶味 只有一股很噁的甜味和水味 為了確認又喝幾口 真的難喝 大家就避開...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>60</td>\n",
       "      <td>基本上在便利商店買這個就是解嘴饞 （因為要吃的鹹酥雞都沒開...） 然後一如其它炸物 微波後...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90</td>\n",
       "      <td>最近好多蘋果氣泡飲料喔 看到綠色瓶身很漂亮，又有嚐鮮價 立刻忍不住試試XD 和其他蘋果汽水比...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100</td>\n",
       "      <td>第一彈其實完全沒有follow到， 偶然在板上看到第二彈情報文，結果覺得更生火！ 身為兒時禮...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>88</td>\n",
       "      <td>早上到小七挑早餐看到紫薯覺得很特別就買來吃吃看 麵包整個顏色偏紫 味道聞起來甜甜 有淡淡的紫...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating                                             review\n",
       "0      30  難喝.. 喝起來沒有抹茶味 只有一股很噁的甜味和水味 為了確認又喝幾口 真的難喝 大家就避開...\n",
       "1      60  基本上在便利商店買這個就是解嘴饞 （因為要吃的鹹酥雞都沒開...） 然後一如其它炸物 微波後...\n",
       "2      90  最近好多蘋果氣泡飲料喔 看到綠色瓶身很漂亮，又有嚐鮮價 立刻忍不住試試XD 和其他蘋果汽水比...\n",
       "3     100  第一彈其實完全沒有follow到， 偶然在板上看到第二彈情報文，結果覺得更生火！ 身為兒時禮...\n",
       "4      88  早上到小七挑早餐看到紫薯覺得很特別就買來吃吃看 麵包整個顏色偏紫 味道聞起來甜甜 有淡淡的紫..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "df1['rating_standard'] = scaler.fit_transform(df['rating'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "      <th>rating_standard</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30</td>\n",
       "      <td>難喝.. 喝起來沒有抹茶味 只有一股很噁的甜味和水味 為了確認又喝幾口 真的難喝 大家就避開...</td>\n",
       "      <td>-2.862466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>60</td>\n",
       "      <td>基本上在便利商店買這個就是解嘴饞 （因為要吃的鹹酥雞都沒開...） 然後一如其它炸物 微波後...</td>\n",
       "      <td>-1.005854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90</td>\n",
       "      <td>最近好多蘋果氣泡飲料喔 看到綠色瓶身很漂亮，又有嚐鮮價 立刻忍不住試試XD 和其他蘋果汽水比...</td>\n",
       "      <td>0.850757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100</td>\n",
       "      <td>第一彈其實完全沒有follow到， 偶然在板上看到第二彈情報文，結果覺得更生火！ 身為兒時禮...</td>\n",
       "      <td>1.469628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>88</td>\n",
       "      <td>早上到小七挑早餐看到紫薯覺得很特別就買來吃吃看 麵包整個顏色偏紫 味道聞起來甜甜 有淡淡的紫...</td>\n",
       "      <td>0.726983</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating                                             review  rating_standard\n",
       "0      30  難喝.. 喝起來沒有抹茶味 只有一股很噁的甜味和水味 為了確認又喝幾口 真的難喝 大家就避開...        -2.862466\n",
       "1      60  基本上在便利商店買這個就是解嘴饞 （因為要吃的鹹酥雞都沒開...） 然後一如其它炸物 微波後...        -1.005854\n",
       "2      90  最近好多蘋果氣泡飲料喔 看到綠色瓶身很漂亮，又有嚐鮮價 立刻忍不住試試XD 和其他蘋果汽水比...         0.850757\n",
       "3     100  第一彈其實完全沒有follow到， 偶然在板上看到第二彈情報文，結果覺得更生火！ 身為兒時禮...         1.469628\n",
       "4      88  早上到小七挑早餐看到紫薯覺得很特別就買來吃吃看 麵包整個顏色偏紫 味道聞起來甜甜 有淡淡的紫...         0.726983"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df1['review'], df1['rating_standard'], test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building and Training\n",
    "Set up and initialize the model, and train it using the Hugging Face Transformers library.\n",
    "\n",
    "1. **Model Selection and Initialization**: Use the pre-trained `bert-base-chinese` tokenizer and the `albert-tiny-chinese-ws` model to process the Chinese reviews.\n",
    "2. **Model Structure Adjustment**: Since this is a regression task, we will modify the model's final layer to be a linear layer for regression.\n",
    "3. **Training and Evaluation**: Set up training arguments, train the model, and evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ckiplab/albert-tiny-chinese-ws and are newly initialized: ['albert.pooler.bias', 'albert.pooler.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at ckiplab/albert-tiny-chinese-ws and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([2, 312]) in the checkpoint and torch.Size([1, 312]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([2]) in the checkpoint and torch.Size([1]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    'ckiplab/albert-tiny-chinese-ws',\n",
    "    num_labels=1,\n",
    "    ignore_mismatched_sizes=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlbertConfig {\n",
       "  \"_attn_implementation_autoset\": true,\n",
       "  \"_name_or_path\": \"ckiplab/albert-tiny-chinese-ws\",\n",
       "  \"architectures\": [\n",
       "    \"AlbertForTokenClassification\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.0,\n",
       "  \"bos_token_id\": 101,\n",
       "  \"classifier_dropout_prob\": 0.1,\n",
       "  \"down_scale_factor\": 1,\n",
       "  \"embedding_size\": 128,\n",
       "  \"eos_token_id\": 102,\n",
       "  \"gap_size\": 0,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.0,\n",
       "  \"hidden_size\": 312,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"LABEL_0\"\n",
       "  },\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"inner_group_num\": 1,\n",
       "  \"intermediate_size\": 1248,\n",
       "  \"label2id\": {\n",
       "    \"LABEL_0\": 0\n",
       "  },\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"albert\",\n",
       "  \"net_structure_type\": 0,\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_groups\": 1,\n",
       "  \"num_hidden_layers\": 4,\n",
       "  \"num_memory_blocks\": 0,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"tokenizer_class\": \"BertTokenizerFast\",\n",
       "  \"transformers_version\": \"4.48.0\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"vocab_size\": 21128\n",
       "}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classifier = torch.nn.Linear(model.config.hidden_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame({'text': X_train, 'label': y_train.astype(float)})\n",
    "test_df = pd.DataFrame({'text': X_test, 'label': y_test.astype(float)})\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b66f1c3215f545f099f56b9b07fede21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8656 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c320e8aa3caf453c8b7c7fe858aba9d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/962 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "test_dataset = test_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results/ckiplab-albert-tiny',\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=16, \n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs/ckiplab-albert-tiny', \n",
    "    logging_steps=100,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\", \n",
    "    load_best_model_at_end=True \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='47' max='5410' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  47/5410 01:50 < 3:39:13, 0.41 it/s, Epoch 0.09/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m writer \u001b[38;5;241m=\u001b[39m SummaryWriter()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(training_args\u001b[38;5;241m.\u001b[39mnum_train_epochs):\n\u001b[0;32m----> 4\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     predictions, labels, _ \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mpredict(test_dataset)\n\u001b[1;32m      8\u001b[0m     mse \u001b[38;5;241m=\u001b[39m mean_squared_error(labels, predictions)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/transformers/trainer.py:2171\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2169\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2170\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2172\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2176\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/transformers/trainer.py:2531\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2524\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2525\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2526\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2527\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2528\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2529\u001b[0m )\n\u001b[1;32m   2530\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2531\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2534\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2535\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2536\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2537\u001b[0m ):\n\u001b[1;32m   2538\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2539\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/transformers/trainer.py:3715\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3712\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_items_in_batch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3713\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n\u001b[0;32m-> 3715\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3717\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/accelerate/accelerator.py:1964\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   1962\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1963\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1964\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter()\n",
    "\n",
    "for epoch in range(training_args.num_train_epochs):\n",
    "    trainer.train()\n",
    "\n",
    "    predictions, labels, _ = trainer.predict(test_dataset)\n",
    "    \n",
    "    mse = mean_squared_error(labels, predictions)\n",
    "    r2 = r2_score(labels, predictions)\n",
    "    writer.add_scalar('MSE', mse, epoch)\n",
    "    writer.add_scalar('R-squared', r2, epoch)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a043ffa66c794aee836178a5b30a914d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5410 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 642.2259, 'grad_norm': 1558.442138671875, 'learning_rate': 1e-05, 'epoch': 0.18}\n",
      "{'loss': 2.5234, 'grad_norm': 12.049978256225586, 'learning_rate': 2e-05, 'epoch': 0.37}\n",
      "{'loss': 1.2878, 'grad_norm': 31.094709396362305, 'learning_rate': 3e-05, 'epoch': 0.55}\n",
      "{'loss': 1.2702, 'grad_norm': 283.6398620605469, 'learning_rate': 4e-05, 'epoch': 0.74}\n",
      "{'loss': 1.1096, 'grad_norm': 21.44092559814453, 'learning_rate': 5e-05, 'epoch': 0.92}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17499ea0aca94198837381c4af93574d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3721643686294556, 'eval_runtime': 10.384, 'eval_samples_per_second': 92.642, 'eval_steps_per_second': 5.874, 'epoch': 1.0}\n",
      "{'loss': 1.0478, 'grad_norm': 26.937990188598633, 'learning_rate': 4.89816700610998e-05, 'epoch': 1.11}\n",
      "{'loss': 1.0277, 'grad_norm': 148.75587463378906, 'learning_rate': 4.79633401221996e-05, 'epoch': 1.29}\n",
      "{'loss': 1.0407, 'grad_norm': 89.99839782714844, 'learning_rate': 4.694501018329939e-05, 'epoch': 1.48}\n",
      "{'loss': 1.0624, 'grad_norm': 87.89129638671875, 'learning_rate': 4.592668024439919e-05, 'epoch': 1.66}\n",
      "{'loss': 1.082, 'grad_norm': 75.37065124511719, 'learning_rate': 4.490835030549899e-05, 'epoch': 1.85}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a45e057bae748b9b68bb6462b58f0ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1064256429672241, 'eval_runtime': 10.343, 'eval_samples_per_second': 93.009, 'eval_steps_per_second': 5.898, 'epoch': 2.0}\n",
      "{'loss': 0.9317, 'grad_norm': 34.96267318725586, 'learning_rate': 4.3890020366598776e-05, 'epoch': 2.03}\n",
      "{'loss': 0.6866, 'grad_norm': 84.91112518310547, 'learning_rate': 4.287169042769857e-05, 'epoch': 2.22}\n",
      "{'loss': 0.7692, 'grad_norm': 75.33419036865234, 'learning_rate': 4.185336048879837e-05, 'epoch': 2.4}\n",
      "{'loss': 0.7399, 'grad_norm': 15.617774963378906, 'learning_rate': 4.083503054989817e-05, 'epoch': 2.59}\n",
      "{'loss': 0.7994, 'grad_norm': 23.839462280273438, 'learning_rate': 3.981670061099796e-05, 'epoch': 2.77}\n",
      "{'loss': 0.8851, 'grad_norm': 20.97551727294922, 'learning_rate': 3.879837067209776e-05, 'epoch': 2.96}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb355b2f17a4461c9c92790ff36141e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7584136724472046, 'eval_runtime': 10.3324, 'eval_samples_per_second': 93.106, 'eval_steps_per_second': 5.904, 'epoch': 3.0}\n",
      "{'loss': 0.7304, 'grad_norm': 21.931194305419922, 'learning_rate': 3.778004073319756e-05, 'epoch': 3.14}\n",
      "{'loss': 0.6167, 'grad_norm': 19.236820220947266, 'learning_rate': 3.676171079429735e-05, 'epoch': 3.33}\n",
      "{'loss': 0.5685, 'grad_norm': 28.828174591064453, 'learning_rate': 3.574338085539715e-05, 'epoch': 3.51}\n",
      "{'loss': 0.5232, 'grad_norm': 47.255088806152344, 'learning_rate': 3.472505091649695e-05, 'epoch': 3.7}\n",
      "{'loss': 0.6247, 'grad_norm': 29.03937339782715, 'learning_rate': 3.370672097759674e-05, 'epoch': 3.88}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4feadb8e25842059ad6d51551a109eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7035555243492126, 'eval_runtime': 10.3456, 'eval_samples_per_second': 92.986, 'eval_steps_per_second': 5.896, 'epoch': 4.0}\n",
      "{'loss': 0.4531, 'grad_norm': 24.594961166381836, 'learning_rate': 3.268839103869654e-05, 'epoch': 4.07}\n",
      "{'loss': 0.4237, 'grad_norm': 35.28298568725586, 'learning_rate': 3.167006109979633e-05, 'epoch': 4.25}\n",
      "{'loss': 0.3864, 'grad_norm': 18.990264892578125, 'learning_rate': 3.065173116089613e-05, 'epoch': 4.44}\n",
      "{'loss': 0.3492, 'grad_norm': 34.4315071105957, 'learning_rate': 2.9633401221995927e-05, 'epoch': 4.62}\n",
      "{'loss': 0.4154, 'grad_norm': 50.49412536621094, 'learning_rate': 2.8615071283095725e-05, 'epoch': 4.81}\n",
      "{'loss': 0.3733, 'grad_norm': 10.11195182800293, 'learning_rate': 2.759674134419552e-05, 'epoch': 4.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13f0e9c9a444438586f9eed03d471f5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7135956883430481, 'eval_runtime': 10.2308, 'eval_samples_per_second': 94.03, 'eval_steps_per_second': 5.962, 'epoch': 5.0}\n",
      "{'loss': 0.2362, 'grad_norm': 18.18777847290039, 'learning_rate': 2.6578411405295317e-05, 'epoch': 5.18}\n",
      "{'loss': 0.2118, 'grad_norm': 42.46464920043945, 'learning_rate': 2.5560081466395115e-05, 'epoch': 5.36}\n",
      "{'loss': 0.2099, 'grad_norm': 16.178770065307617, 'learning_rate': 2.454175152749491e-05, 'epoch': 5.55}\n",
      "{'loss': 0.2321, 'grad_norm': 15.038445472717285, 'learning_rate': 2.3523421588594704e-05, 'epoch': 5.73}\n",
      "{'loss': 0.2672, 'grad_norm': 61.39918899536133, 'learning_rate': 2.2505091649694502e-05, 'epoch': 5.91}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75d28879a787421387afd8c44ce904ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.688459038734436, 'eval_runtime': 10.2452, 'eval_samples_per_second': 93.897, 'eval_steps_per_second': 5.954, 'epoch': 6.0}\n",
      "{'loss': 0.1554, 'grad_norm': 10.53122329711914, 'learning_rate': 2.1486761710794297e-05, 'epoch': 6.1}\n",
      "{'loss': 0.1611, 'grad_norm': 11.638997077941895, 'learning_rate': 2.0468431771894095e-05, 'epoch': 6.28}\n",
      "{'loss': 0.1907, 'grad_norm': 9.239984512329102, 'learning_rate': 1.9450101832993893e-05, 'epoch': 6.47}\n",
      "{'loss': 0.1372, 'grad_norm': 43.03913116455078, 'learning_rate': 1.8431771894093687e-05, 'epoch': 6.65}\n",
      "{'loss': 0.1345, 'grad_norm': 26.116607666015625, 'learning_rate': 1.741344195519348e-05, 'epoch': 6.84}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d86784da3df141d0977a59450a88fb7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7480412721633911, 'eval_runtime': 10.2495, 'eval_samples_per_second': 93.858, 'eval_steps_per_second': 5.952, 'epoch': 7.0}\n",
      "{'loss': 0.1205, 'grad_norm': 12.522576332092285, 'learning_rate': 1.639511201629328e-05, 'epoch': 7.02}\n",
      "{'loss': 0.09, 'grad_norm': 6.918332576751709, 'learning_rate': 1.5376782077393077e-05, 'epoch': 7.21}\n",
      "{'loss': 0.091, 'grad_norm': 10.191756248474121, 'learning_rate': 1.4358452138492872e-05, 'epoch': 7.39}\n",
      "{'loss': 0.0829, 'grad_norm': 20.507282257080078, 'learning_rate': 1.334012219959267e-05, 'epoch': 7.58}\n",
      "{'loss': 0.0805, 'grad_norm': 9.436700820922852, 'learning_rate': 1.2321792260692464e-05, 'epoch': 7.76}\n",
      "{'loss': 0.0779, 'grad_norm': 17.215713500976562, 'learning_rate': 1.1303462321792262e-05, 'epoch': 7.95}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ecbfb3db8764d348eb3479c5941a283",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7254511713981628, 'eval_runtime': 10.8211, 'eval_samples_per_second': 88.9, 'eval_steps_per_second': 5.637, 'epoch': 8.0}\n",
      "{'loss': 0.0581, 'grad_norm': 6.289877891540527, 'learning_rate': 1.0285132382892057e-05, 'epoch': 8.13}\n",
      "{'loss': 0.0497, 'grad_norm': 7.017858028411865, 'learning_rate': 9.266802443991853e-06, 'epoch': 8.32}\n",
      "{'loss': 0.054, 'grad_norm': 12.040273666381836, 'learning_rate': 8.248472505091651e-06, 'epoch': 8.5}\n",
      "{'loss': 0.0476, 'grad_norm': 10.282316207885742, 'learning_rate': 7.230142566191446e-06, 'epoch': 8.69}\n",
      "{'loss': 0.0481, 'grad_norm': 23.71063232421875, 'learning_rate': 6.211812627291243e-06, 'epoch': 8.87}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24e69ddc8e32429085d98fb2b3bb0897",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7342021465301514, 'eval_runtime': 10.8341, 'eval_samples_per_second': 88.794, 'eval_steps_per_second': 5.63, 'epoch': 9.0}\n",
      "{'loss': 0.0426, 'grad_norm': 5.722010135650635, 'learning_rate': 5.193482688391039e-06, 'epoch': 9.06}\n",
      "{'loss': 0.0262, 'grad_norm': 5.641330242156982, 'learning_rate': 4.175152749490835e-06, 'epoch': 9.24}\n",
      "{'loss': 0.0356, 'grad_norm': 5.461765766143799, 'learning_rate': 3.1568228105906318e-06, 'epoch': 9.43}\n",
      "{'loss': 0.0279, 'grad_norm': 4.584555625915527, 'learning_rate': 2.1384928716904276e-06, 'epoch': 9.61}\n",
      "{'loss': 0.0244, 'grad_norm': 8.97751235961914, 'learning_rate': 1.120162932790224e-06, 'epoch': 9.8}\n",
      "{'loss': 0.0246, 'grad_norm': 5.923080921173096, 'learning_rate': 1.0183299389002036e-07, 'epoch': 9.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb179d03a7ff49d488fec02cb1d2f1b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7300607562065125, 'eval_runtime': 10.3447, 'eval_samples_per_second': 92.994, 'eval_steps_per_second': 5.897, 'epoch': 10.0}\n",
      "{'train_runtime': 2568.029, 'train_samples_per_second': 33.707, 'train_steps_per_second': 2.107, 'train_loss': 12.326706225296926, 'epoch': 10.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5410, training_loss=12.326706225296926, metrics={'train_runtime': 2568.029, 'train_samples_per_second': 33.707, 'train_steps_per_second': 2.107, 'total_flos': 348519843594240.0, 'train_loss': 12.326706225296926, 'epoch': 10.0})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "286aaa68787a42809876c409f083a03a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.688459038734436\n"
     ]
    }
   ],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(f\"Validation Loss: {eval_results['eval_loss']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f075523021e4fc8ada3cc623d90e49d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions, labels, _ = trainer.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mse = mean_squared_error(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
